# ACGAN implementation on MNIST

### 1. Train the model
```
python model.py
``` 
### 2. Train the model2
```
python model2.py
```
### 3. See images
```
python see_images.py
```
this file also saves the images that you see, and will only work for models generated from training model2.py
### 4. Generate video
```
python make_video.py
```
this generates video from the images generated by see_images.py

---

## Description

The model.py is an implementation of the acgan as suggested in the paper. The generator takes two inputs, _noise_ and the _class label_. Then a trainable _embedding_ layer is applied and then it is mutiplied with the noise. 

The model2.py is an effort to improve the mapping of _generator_ in a certain way, _i.e._ to transform the _mapping_ of the _generator_ from discrete to a continuous one. So instead of giving just an integer as _class label_, we give a vector in which we give the respective _class intent_ of each of the classes where all the intents of the respective classes sum to one. Though as I have seen, it does not give any promising performance if more than two classes are intended. 

In the above problem, as the _embedding_ is being defined externally, a _fully connected_ layer is introduced which learns the representation of the _class intent_.

The rest of the model remains the same for both the implementations which is as mentioned in the [paper](https://arxiv.org/pdf/1610.09585.pdf).
